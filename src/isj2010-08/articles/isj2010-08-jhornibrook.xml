<?xml version="1.0" encoding="UTF-8"?>
<docbook:article xmlns:docbook="http://docbook.org/ns/docbook"
	xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xml="http://www.w3.org/XML/1998/namespace"
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://docbook.org/ns/docbook docbook.xsd">
	<docbook:info>
		<docbook:title>Don’t Flip Out! How to Stop Your Query Access Plans from Flopping</docbook:title>
		<docbook:author>
			<docbook:personname>John Hornibrook</docbook:personname>
		</docbook:author>
		<docbook:author>
			<docbook:personname>Editors : Guy Lohman, Terry Purcell</docbook:personname>
		</docbook:author>
	</docbook:info>
	<docbook:para>
Has this ever happened to you? You have an important report query that has been running on an IBM® DB2® for Linux®, UNIX®, and Windows® data 
server consistently in 25 seconds for the past year. Catalog statistics are regularly collected for tables referenced in the query using the 
RUNSTATS utility. Suddenly, one day this report query takes 250 seconds to execute after statistics have been collected. Or another query in 
the same reporting application consistently executes in 15 seconds. A small change is made to one of the search conditions in this query that 
doesn’t significantly increase the size of the result set, and execution now takes 500 seconds! After following standard performance problem 
determination procedures, you discover that the access plan has changed in both scenarios. Now what do you do?
	</docbook:para>
	<docbook:para>
This edition of the Intelligent Optimizer will discuss some of the reasons why access plans occasionally change for the worse and what you can 
do to prevent access plan flip-flop from happening.
	</docbook:para>
	<docbook:section>
		<docbook:title>A Reactive Approach</docbook:title>
		<docbook:para>
Once you’ve determined that a query’s performance degradation is due to an access plan change, you can compare the differences to try to 
understand why the DB2 data server optimizer chose a new access plan. The explain facility is the recommended tool for capturing access plans. 
The access plan’s details are written to a set of explain tables. You can use the db2exfmt tool to format the contents of the explain tables 
to produce a text file, or you can use the Data Studio Visual Explain tool to provide a graphical view of the access plan. You can read more 
about the explain facility 
			<docbook:link xlink:href="http://publib.boulder.ibm.com/infocenter/db2luw/v9r7/topic/com.ibm.db2.luw.admin.perf.doc/doc/c0005134.html">here</docbook:link>
		</docbook:para>
		<docbook:para>
The explain facility provides detailed information about the access plan, including each access plan operator, their sequence of execution, 
the predicates they apply, the number of rows they process, and their estimated cost. The number of rows processed by an access plan operator 
directly affects its cost. Sometimes the optimizer mis-estimates the number of rows – or cardinality – because of missing or outdated catalog 
statistics. The inaccurate cardinality estimates may result in inappropriate access plan changes, so verifying them is often a good place for 
you to start. The cardinality estimates are based on the number of rows in each referenced database object (table, nickname, table function or 
materialized query table) and the predicates applied to those objects. It is possible that the cardinality catalog statistic for a base object 
is inaccurate or it is possible that the selectivity estimate for one or more of the predicates applied to the base object is inaccurate. You 
can verify the optimizer’s cardinality estimates by running subsets of the original query to return a count of the actual number of rows 
processed by specific access plan operators. Alternatively, if DB2 9.7 is being used, you can enable the DB2 data server to automatically 
capture the actual runtime cardinality of each operator and store them in the explain tables, where you can later format the contents using 
db2exfmt1. You can read more about capturing actual runtime cardinalities 
			<docbook:link xlink:href="http://publib.boulder.ibm.com/infocenter/db2luw/v9r7/topic/com.ibm.db2.luw.admin.perf.doc/doc/c0056362.html">here</docbook:link>
		</docbook:para>
		<docbook:para>	
Once you’ve identified and corrected any inaccurate cardinality estimates - where that is possible - the access plan may still not have changed, 
or it may have changed but is still not performing as well as the original access plan. You will then need to perform deeper analysis to 
understand why the optimizer is still not choosing the better access plan.
		</docbook:para>
		<docbook:para>
As you can appreciate by this point, analyzing poorly performing access plans requires considerable time and skill. While access plan analysis 
skills are very valuable for you to have, performing such a deep analysis for more than a relatively small number of query access plan changes 
is probably impractical. Quite often, access plan problems need to be corrected quickly, and there isn’t time to perform the steps described 
above. So a more proactive approach is preferable.
		</docbook:para>
	</docbook:section>

	<docbook:section>
		<docbook:title>A Proactive Approach</docbook:title>
		<docbook:para>
Rather than scrambling to fix query access plan problems in panic situations, there are a number of modifications that you can make to your 
SQL statements, data server configuration, and catalog statistics, to avoid unexpected access plan changes. These modifications allow the 
optimizer to more accurately cost access plans so that queries run faster. Moreover, when access plan cost estimates are accurate, access 
plans are less likely to change unexpectedly for the worse, when some aspect of the environment that is considered by the optimizer’s cost 
model changes. For example, a small increase in the number of rows in a table should not result in a new access plan that runs significantly 
slower. Access plans usually change incorrectly in this type of situation because of other costing inaccuracies. In other words, the 
optimizer’s view of the world is distorted, so it is making decisions based on bad data. The following sections in this article describe how 
you can improve the optimizer’s “vision” so it has a clear view of the best access path.
		</docbook:para>
	</docbook:section>

	<docbook:section>
		<docbook:title>Writing SQL Statements</docbook:title>
		<docbook:para>
SQL is a powerful language that enables you to specify relational expressions in syntactically different but semantically equivalent ways. 
However, some semantically equivalent variations are easier to optimize than others. Although the DB2 data server optimizer has a powerful 
query rewrite capability, it might not always be able to rewrite an SQL statement into the most optimal form. There are also certain SQL 
constructs that can limit the access plans considered by the query optimizer. For example, complex expressions in predicates can limit 
the optimizer’s ability to compute accurate selectivity estimates and can prevent certain access plan operators from being used. The following 
are some examples of such predicates:-
			<docbook:orderedlist>
				<docbook:listitem>
					<docbook:programlisting>WHERE SALES.PRICE * SALES.DISCOUNT = TRANS.FINAL_PRICE</docbook:programlisting>
					</docbook:listitem>
				<docbook:listitem>
					<docbook:programlisting>WHERE UPPER(CUST.LASTNAME) = TRANS.NAME</docbook:programlisting>
				</docbook:listitem>
				<docbook:listitem>
					<docbook:programlisting>WHERE INTEGER(TRANS_DATE)/100 = 200802</docbook:programlisting>
				</docbook:listitem>
			</docbook:orderedlist>
		</docbook:para>
		<docbook:para>	
All of these predicates may prevent the optimizer from computing an accurate selectivity estimate. The first two predicates will limit the 
join method to nested-loop join because hash join and merge join cannot be used if the join predicate contains an expression. The third 
predicate cannot be applied by an index on the TRANS_DATE column using start/stop keys. 
		</docbook:para>
		<docbook:para>
One approach you can use to avoid expressions in predicates is to materialize the expression in the table as a generated column. An index can 
then be created on the column. For example:
			<docbook:programlisting>
CREATE TABLE CUSTOMER 
(LASTNAME   VARCHAR(100), 
 U_LASTNAME VARCHAR(100) 
   GENERATED ALWAYS AS (UCASE(LASTNAME))
);

CREATE INDEX CUST_U_LASTNAME ON CUSTOMER(U_LASTNAME)
			</docbook:programlisting>
		</docbook:para>
		<docbook:para>	
Another approach is to use the inverse of the expression, to avoid referencing a column in the expression. With this approach, the column is 
referenced alone on one side of the predicate. For example, you can write the third predicate above as:
			<docbook:programlisting>
WHERE TRANS_DATE BETWEEN 20080201 AND 20080229
			</docbook:programlisting>
		</docbook:para>
		<docbook:para>	
This allows the BETWEEN predicate to be applied as a start and stop key on an index on TRANS_DATE.
		</docbook:para>
		<docbook:para>
			<docbook:link xlink:href="http://publib.boulder.ibm.com/infocenter/db2luw/v9r7/topic/com.ibm.db2.luw.admin.perf.doc/doc/c0054707.html">This section</docbook:link>
of the DB2 9.7 Information Center contains more information about best practices for writing good SQL statements.
		</docbook:para>
		<docbook:para>
Another potential problem for query optimization is the use of input variables, such as host variables and parameter markers, in predicates, 
rather than literals. For example:
			<docbook:programlisting>WHERE TRANS_DATE BETWEEN ? AND ?</docbook:programlisting>
		</docbook:para>
		<docbook:para>	
When literals are specified in the predicate, the optimizer can compare them to frequent value or histogram statistics to compute a more 
accurate selectivity estimate. Using these distribution statistics will result in a more accurate selectivity estimate when the data is 
skewed or when range predicates are used. 
		</docbook:para>
		<docbook:para>
Input variables are essential for good statement preparation time in an online transaction processing (OLTP) environment, where statements 
tend to be simpler and query access plan selection is more straightforward. Multiple executions of the same query with different input 
variable values can reuse the compiled access section in the dynamic statement cache, avoiding expensive SQL statement compilations whenever 
the input values change. 
		</docbook:para>
		<docbook:para>
However, for complex queries, input variables can cause problems because plan selection is more complex and the optimizer needs more information 
to make the best decisions. Moreover, for complex queries, statement compilation time is usually a small component of total execution time, and 
complex queries do not tend to be repeated with exactly the same predicates, so they do not benefit from the dynamic statement cache.
		</docbook:para>
		<docbook:para>
If input variables need to be used in a complex query workload, consider using the REOPT bind option. The REOPT bind option defers statement 
compilation from PREPARE to OPEN or EXECUTE time, when the input variable values are known. The values are passed to the optimizer so it can 
use them to compute a more accurate selectivity estimate. The REOPT bind option can also be used for predicates that reference special 
registers, such as:
			<docbook:programlisting>WHERE TRANS_DATE &lt; CURRENT DATE</docbook:programlisting>
		</docbook:para>
		<docbook:para>
			<docbook:link xlink:href="http://publib.boulder.ibm.com/infocenter/db2luw/v9r7/topic/com.ibm.db2.luw.admin.perf.doc/doc/c0055082.html">This Information Center page</docbook:link>	
contains more details on how to use the REOPT bind option, for different types of applications.
		</docbook:para>
	</docbook:section>

	<docbook:section>
		<docbook:title>Data Server Configuration</docbook:title>
		<docbook:para>
There are a number of configuration options that affect the DB2 data server’s performance, such as database and database manager configuration 
parameters, registry variables, bind options, special registers, and database object attributes. Some of these configuration options have a 
direct effect on the DB2 data server optimizer because they provide information about the system’s capabilities that is considered when 
estimating the cost of access plans. Therefore, access plans will tend to be more stable if configuration options important to the optimizer’s 
cost model are set accurately. 
		</docbook:para>
	</docbook:section>
	
	<docbook:section>
		<docbook:title>Configuration Parameters</docbook:title>
		<docbook:para>
The following two database manager configuration parameters are used by the optimizer’s cost model. While both are set automatically by the DB2 
data server, you should review them to ensure that they are set to appropriate values for your environment.
			<docbook:variablelist>
				<docbook:varlistentry>
					<docbook:term>CPUSPEED</docbook:term>
					<docbook:listitem>
The average time to execute a machine instruction in milliseconds. This value is automatically computed by the DB2 data server. It can also be 
set manually.
					</docbook:listitem>
				</docbook:varlistentry>
				<docbook:varlistentry>
					<docbook:term>COMM_BANDWIDTH</docbook:term>
					<docbook:listitem>
The communication bandwidth between database partition servers in megabytes per second. This value is automatically computed by the DB2 data 
server based on the speed of the underlying communications adapter.  This value is used by the query optimizer to estimate the cost of 
transferring data between the database partition servers of a partitioned database system.
					</docbook:listitem>
				</docbook:varlistentry>
			</docbook:variablelist>
The optimizer’s cost model also considers the size of the buffer pools and the amount of available sortheap memory. The cost model also 
considers the amount of memory available for acquiring locks, so that it can chose the best lock granularity appropriate for the specified 
isolation level. The size of these memory resources should be determined based on available system memory and the type of workload running 
on your data server. You can specify the size of these memory resources yourself, or you can let the DB2 data server do it for you, using the 
self-tuning memory feature. Whatever approach you choose, you should be aware that the size of these memory resources is considered by the 
optimizer too.
		</docbook:para>
		<docbook:para>
The AVG_APPLS database configuration parameter is used by the query optimizer to help estimate how much buffer pool will be available at 
runtime for the chosen access plan. This parameter is only used by the optimizer’s cost model and doesn’t directly affect the allocation of 
system resources at query execution time. When running the DB2 data server in a multi-user environment, particularly with complex queries and 
a large buffer pool, this parameter informs the query optimizer that multiple query users will be using your system, so that the optimizer 
should be more conservative in its assumptions of buffer pool availability. However, setting this parameter too high may result in access 
plans that favor using more CPU or sortheap, due to incorrectly estimating that extra I/O will occur because of insufficient buffer pool 
memory. It is recommended that this parameter not be set higher than 3 or 5, if it is even necessary to set it at all. 
		</docbook:para>
	</docbook:section>
	<docbook:section>
		<docbook:title>Table Space Attributes</docbook:title>
		<docbook:para>
The optimizer also considers storage subsystem characteristics in the cost model. The storage subsystem characteristics are represented as 
table space attributes. You can specify table space attributes using the CREATE or ALTER TABLESPACE statements. The optimizer considers the 
following table space attributes. 
		<docbook:variablelist>
			<docbook:varlistentry>
				<docbook:term>PREFETCHSIZE</docbook:term>
				<docbook:listitem>The number of pages to be read when prefetching is performed.</docbook:listitem>
			</docbook:varlistentry>
			<docbook:varlistentry>
				<docbook:term>EXTENTSIZE</docbook:term>
				<docbook:listitem>The size of each extent in pages. This many pages are written to one container in the table space before 
				switching to the next container. This parameter can only be specified when a table space is created.
				</docbook:listitem>
			</docbook:varlistentry>
			<docbook:varlistentry>
				<docbook:term>OVERHEAD</docbook:term>
				<docbook:listitem>Provides an estimate of the time in milliseconds that is required by a table space container before any data 
				is read into memory. This overhead activity includes the container's I/O controller overhead as well as the disk latency time, 
				which includes the disk seek time. 
				</docbook:listitem>
			</docbook:varlistentry>
			<docbook:varlistentry>
				<docbook:term>TRANSFERRATE</docbook:term>
				<docbook:listitem>Provides an estimate of the time in milliseconds that is required to read one page of data into memory.</docbook:listitem>
			</docbook:varlistentry>
		</docbook:variablelist>		
The DB2 data server doesn’t automatically determine the table space overhead and transfer rate. If you don’t specify values when the table space
is created, default values are used by the optimizer. However they may not be representative of your storage subsystem. I/O can be the most 
significant cost of an access plan, so it is important for you to set these parameters accurately. 			
		<docbook:link xlink:href="http://publib.boulder.ibm.com/infocenter/db2luw/v9r7/topic/com.ibm.db2.luw.admin.perf.doc/doc/c0005051.html">This Information Center page</docbook:link>	
contains information on how to set these parameters accurately.
		</docbook:para>
	</docbook:section>

	<docbook:section>
		<docbook:title>Optimization Class</docbook:title>
		<docbook:para>
The DB2 data server allows you to control the level of optimization performed by the query optimizer. Different levels of optimization are 
grouped into classes. You can specify the query optimization class using the CURRENT QUERY OPTIMIZATION special register or the QUERYOPT bind 
option. Setting the optimization class can provide some of the advantages of explicitly specifying optimization techniques, particularly for 
the following reasons:
		<docbook:itemizedlist>
			<docbook:listitem>To manage very small databases or very simple dynamic queries</docbook:listitem> 
			<docbook:listitem>To accommodate memory limitations at compile time on your database server</docbook:listitem>
			<docbook:listitem>To reduce the query compilation time, such as PREPARE.</docbook:listitem>
		</docbook:itemizedlist>	 
Most statements can be adequately optimized with a reasonable amount of resources by using optimization class 5, which is the default query 
optimization class. At a given optimization class, the query compilation time and resource consumption is primarily influenced by the 
complexity of the query, particularly the number of joins and subqueries. However, compilation time and resource usage are also affected by 
the amount of optimization performed.
		</docbook:para>
		<docbook:para>
Query optimization classes 1, 2, 3, 5, and 7 are all suitable for general-purpose use. Consider class 0 only if you require further reductions 
in query compilation time and you know that the SQL statements are extremely simple. 
		</docbook:para>
		<docbook:para>
Be aware that if you choose an optimization class that is too low for your workload, the estimated cost of the access plans may be inaccurate 
because certain types of statistics aren’t used. In particular, frequent value and histogram statistics aren’t used at optimization class 0 
and 1 and histogram statistics aren’t used at optimization class 3. 
<docbook:link xlink:href="http://publib.boulder.ibm.com/infocenter/db2luw/v9r7/topic/com.ibm.db2.luw.admin.perf.doc/doc/c0005277.html">
This DB2 Information Center page</docbook:link> provides more details on the different optimization classes and how to choose the best 
optimization class for your workload.
		</docbook:para>
	</docbook:section>	

	<docbook:section>
		<docbook:title>Catalog Statistics</docbook:title>
		<docbook:para>
The single most important action that you can take to stabilize your access plans is to ensure that accurate catalog statistics are available. 
You can collect statistics by issuing the RUNSTATS command or you can configure the DB2 data server to automatically collect statistics for 
you. You can also request for statistics to be collected when using the LOAD, CREATE INDEX, or REDISTRIBUTE utilities. 
		</docbook:para>
		<docbook:para>
If collecting statistics manually using the RUNSTATS command, you should use the following options at a minimum:
			<docbook:programlisting>
RUNSTATS ON TABLE DB2USER.DAILY_SALES WITH DISTRIBUTION 
AND SAMPLED DETAILED INDEXES ALL;
			</docbook:programlisting>
Distribution statistics make the optimizer aware of data skew. Detailed index statistics provide more details about the I/O required to fetch 
data pages when the table is accessed using a particular index. Collecting detailed index statistics consumes considerable CPU and memory 
for large tables. The SAMPLED option provides detailed index statistics with nearly the same accuracy but requires a fraction of the CPU and 
memory. These options are also used by automatic statistics collection when a statistical profile has not been provided for a table.
		</docbook:para>
		<docbook:para>
To further improve query performance and access plan stability, you should consider collecting more advanced statistics such as column group 
cardinalities, LIKE statistics or creating statistical views.
		</docbook:para>
	</docbook:section>
	
	<docbook:section>
		<docbook:title>Column Group Statistics</docbook:title>
		<docbook:para>
Column group cardinality statistics represent the number of distinct values in a group of columns. This statistic helps the optimizer 
recognize when the data in columns is correlated. Without it, the optimizer assumes that the data in columns is independent. For example, 
consider an automobile table containing a MAKE and MODEL column with 20 and 200 distinct values, respectively. Assuming each value of MAKE 
and MODEL occurs uniformly, the optimizer will compute the combined selectivity estimate of these predicates:
			<docbook:programlisting>
WHERE MAKE=’HONDA’ AND MODEL=’ODYSSEY’
			</docbook:programlisting>
as (1/200) * (1/20) = 1/4000. However, it is unlikely that a particular model of automobile occurs for more than one make, so the number of 
distinct make and model combinations is likely to be 200 and the combined selectivity of these predicates is 1/200. You can collect the 
column group cardinality for these columns using this RUNSTATS command:
			<docbook:programlisting>
RUNSTATS ON TABLE DB2USER.AUTOS ON ALL COLUMNS AND COLUMNS((MAKE,MODEL)) 
WITH DISTRIBUTION AND SAMPLED DETAILED INDEXES ALL;
			</docbook:programlisting>
		</docbook:para>
	</docbook:section>
	
	<docbook:section>
		<docbook:title>LIKE Predicate Statistics</docbook:title>
		<docbook:para>
If you specify LIKE predicates using the % wildcard character in any position other than at the end of the pattern, you should collect basic 
information about the sub-element structure of the strings in your data. For example:
			<docbook:programlisting>
WHERE KEYWORDS LIKE '%simulation%'
			</docbook:programlisting>
		</docbook:para>
		<docbook:para>	
Table columns should contain sub-fields or sub-elements separated by blanks. For example, a four-row table DOCUMENTS contains a KEYWORDS 
column with lists of relevant keywords for text retrieval purposes. The values in KEYWORDS are:
			<docbook:programlisting>
'database simulation analytical business intelligence'
'simulation model fruit fly reproduction temperature'
'forestry spruce soil erosion rainfall'
'forest temperature soil precipitation fire'
			</docbook:programlisting>
		</docbook:para>	
		<docbook:para>
In this example, each column value consists of 5 sub-elements, each of which is a word (the keyword), separated from the others by one blank.
		</docbook:para>
		<docbook:para>
When the % wildcard character appears in some position other than at the end of the pattern, the optimizer assumes that the column being 
matched contains a series of elements concatenated together, and it estimates the length of each element based on the length of the string, 
excluding leading and trailing % characters. If you collect sub-element statistics, the optimizer will have information about the length or 
each sub-element and the delimiter. It can use this additional information to more accurately estimate how many rows will match the predicate.
		</docbook:para>
		<docbook:para>
To collect sub-element statistics, execute RUNSTATS with the LIKE STATISTICS clause. 
		</docbook:para>
	</docbook:section>	

	<docbook:section>
		<docbook:title>Statistical Views</docbook:title>
		<docbook:para>
The statistics described above are all important for computing an accurate cardinality estimate; however there are some situations where more 
sophisticated statistics are required. In particular, more sophisticated statistics are required to represent more complex relationships, such 
as predicates involving expressions:
			<docbook:programlisting>WHERE PRICE > MSRP + MARKUP</docbook:programlisting>
relationships spanning multiple tables:
			<docbook:programlisting>WHERE PRODUCT.NAME=’BBQ’ AND PRODUCT.PRODKEY = DAILY_SALES.PRODKEY</docbook:programlisting>
or anything other than predicates involving independent attributes and simple comparison operations. Statistical views are able to represent 
these types of complex relationships because statistics are collected on the result set returned by the view, rather than the base tables 
referenced by the view. 
		</docbook:para>
		<docbook:para>
When a query is compiled, the optimizer automatically tries to match the query to the available statistical views. When the optimizer computes 
cardinality estimates for intermediate result sets, it uses the statistics from matching views to compute a better estimate. 
		</docbook:para>
		<docbook:para>
Queries do not need to reference the statistical view directly in order for the optimizer to use it. The optimizer uses the same matching 
mechanism that is used for materialized query tables (MQTs) to match queries automatically to statistical views. In this respect, statistical 
views are very similar to MQTs, except they are not stored permanently, so they do not consume disk space, and do not have to be maintained.
		</docbook:para>
		<docbook:para>
However, you do have to decide what statistical views to create, and create them.  You create a statistical view by first creating a view and 
then enabling it for optimization using the ALTER VIEW statement. You then issue RUNSTATS on the statistical view. For example, in order to 
create a statistical view to represent the join between the time dimension table and the fact table in a star schema, you would do the 
following:
			<docbook:programlisting>
CREATE VIEW SV_TIME_FACT AS 
(SELECT T.* FROM TIME T, DAILY_SALES S
 WHERE T.TIMEKEY = S.TIMEKEY
);

ALTER VIEW SV_TIME_FACT ENABLE QUERY OPTIMIZATION;

RUNSTATS ON TABLE DB2DBA.SV_TIME_FACT WITH DISTRIBUTION;
			</docbook:programlisting>
		</docbook:para>
		<docbook:para>
This statistical view can be used to improve the cardinality estimate - and consequently the access plan and query performance - for queries 
such as:
			<docbook:programlisting>
SELECT SUM(S.PRICE)  
FROM DAILY_SALES S, TIME T, PRODUCT P
WHERE 
T.TIMEKEY = S.TIMEKEY AND T.YEAR_MON = 200712 AND 
P.PRODKEY = S.PRODKEY AND P.PROD_DESC = ‘Power drill’;
			</docbook:programlisting>
		</docbook:para>
		<docbook:para>
Without the statistical view, the optimizer assumes that all fact table TIMEKEY values corresponding to a particular time dimension YEAR_MON 
value occur uniformly within the fact table. However, sales might have been particularly strong in December, resulting in many more sales 
transactions than other months.
		</docbook:para>
		<docbook:para>
There are many situations where statistical views can improve query performance and improve access plan stability. There are some 
straightforward best practices available to help you determine what statistical views to create. You can find more information about 
how statistical views work and when to create them in 
<docbook:link xlink:href="http://publib.boulder.ibm.com/infocenter/db2luw/v9r7/topic/com.ibm.db2.luw.admin.perf.doc/doc/c0021713.html">
this DB2 Information Center page.
</docbook:link> .
		</docbook:para>
	</docbook:section>

	<docbook:section>
		<docbook:title>Handling Exceptions</docbook:title>
		<docbook:para>
Unfortunately, occasionally access plans continue to unexpectedly change for the worse, even though you’ve taken the tuning actions described 
above. Sometimes it is necessary for you to override the optimizer’s decisions, in order to quickly correct a poorly performing access plan or 
to reduce the risk of an existing access plan changing. If you’ve properly tuned your SQL statements and your DB2 data server, you should only 
need to override the optimizer in exceptional situations. Nonetheless, it is important for you to be aware of the different ways to override 
the optimizer, so that you can react quickly in emergency situations, or so that you can prevent emergency situations from occurring at all. 
		</docbook:para>
	</docbook:section>
	
	<docbook:section>	
		<docbook:title>Access Plan Reuse</docbook:title>
		<docbook:para>
Access plan reuse is a new feature available in DB2 9.7 that allows you to request that the access plans for static SQL statements be preserved 
across binds or rebinds. 
		</docbook:para>
		<docbook:para>
In some situations, you might choose to rebind your packages in order to update the access plans to reflect changes in catalog statistics or 
changes in the DB2 data server configuration. However, sometimes existing packages need to be rebound because of functional changes, such as a 
release upgrade or the installation of a new service level, but you don’t want the existing access plans to change. When a new DB2 data server 
release is installed, existing packages must be rebound in order for access sections to be rebuilt, so that they are compatible with the 
runtime component of the new data server release. When a new service level is installed, existing packages don’t need to be rebound because 
access sections are compatible across service levels for the same release. However, you may want to rebuild existing access sections, to take 
advantage of runtime improvements in the new service level. 
		</docbook:para>
		<docbook:para>
Sometimes existing packages need to be rebound because of changes in database objects referenced by SQL statements in the packages. Changes to 
the database objects may have implicitly invalidated existing packages, so they must be rebound before they can be used. You can rebind 
invalidated packages yourself using the BIND or REBIND commands or you can let the DB2 data server automatically rebind the packages the next 
time the application attempts to use the package.
		</docbook:para>
		<docbook:para>
Ideally, access plans should change for the better when a package is rebound, but sometimes you cannot take that risk, because you aren’t able 
to react fast enough to unexpected access plan changes.
		</docbook:para>
		<docbook:para>
You can enable access plan reuse through the ALTER PACKAGE statement, or by using the APREUSE option on the BIND, REBIND, or PRECOMPILE command. 
Packages that are subject to access plan reuse have the value ‘Y’ in the APREUSE column of the SYSCAT.PACKAGES catalog view.
		</docbook:para>
		<docbook:para>
The ALTER_ROUTINE_PACKAGE procedure is a convenient way for you to enable access plan reuse for compiled SQL objects, such as routines, 
functions, and triggers. 
		</docbook:para>
		<docbook:para>
Access plan reuse is most effective when changes to the schema and compilation environment are kept to a minimum. 
If significant changes are made, it might not be possible to recreate the previous access plan. Examples of such significant changes include 
dropping an index that is being used in an access plan, or recompiling an SQL statement at a different optimization level. Significant changes 
to the query compiler's analysis of the statement can also result in the previous access plan no longer being reusable.
		</docbook:para>
		<docbook:para>
Access plans from packages that were produced by releases prior to DB2 9.7 cannot be reused.
		</docbook:para>
		<docbook:para>
If an access plan cannot be reused, compilation continues, but a warning (SQL20516W) is returned with a reason code that indicates why the 
attempt to reuse the access plan was not successful. Additional information is sometimes provided in the diagnostic messages that are available 
through the explain facility.
		</docbook:para>
		<docbook:para>
You can read more about access plan reuse in this <docbook:link xlink:href="http://publib.boulder.ibm.com/infocenter/db2luw/v9r7/topic/com.ibm.db2.luw.admin.perf.doc/doc/c0055383.html">DB2 Information Center page</docbook:link>.
		</docbook:para>
	</docbook:section>
	
	<docbook:section>
		<docbook:title>Optimization Profiles</docbook:title>
		<docbook:para>
The DB2 data server supports an even more direct way to influence access plans using optimization profiles. Optimization profiles allow you to 
specify access plan details such as base access and join methods and join order. For example, you can specify that access to a particular 
table should use a particular index or you can specify that two tables should be joined using the hash join method. Optimization profiles also 
allow you to control query rewrite optimizations such transforming certain types of subqueries to joins. You can specify the base table access 
methods, join methods, and join order for the entire access plan, or just a subset of the access plan.
		</docbook:para>
		<docbook:para>
An optimization profile is specified as an XML document that you create and store in the SYSTOOLS.OPT_PROFILE table. You put an optimization 
profile into effect for your application by specifying the optimization profile name using the CURRENT OPTIMIZATION PROFILE special register 
for dynamic SQL statements, or by specifying the OPTPROFILE bind option for static SQL statements.
		</docbook:para>
		<docbook:para>
An optimization profile contains optimization guidelines that specify the access plan details. An optimization profile can contain optimization 
guidelines for one or more SQL statements. The SQL statement text is stored in the optimization profile along with the optimization guidelines. 
When an optimization profile is in effect for your application, each SQL statement compiled by your application will be matched to the SQL 
statements specified in the optimization profile. When a matching SQL statement is found in the optimization profile, the SQL compiler will use 
the optimization guidelines for that SQL statement while optimizing it.
		</docbook:para>
		<docbook:example>
			<docbook:title>An example of an optimization profile</docbook:title>
			<docbook:programlisting>
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;OPTPROFILE VERSION=“9.7.0"&gt;
&lt;!--
	Global optimization guidelines section. Optional but at most one.
--&gt;
&lt;OPTGUIDELINES&gt;
	&lt;RTS TIME=“1000"/&gt;
&lt;/OPTGUIDELINES&gt;
&lt;!--
	Statement profile section. Zero or more.
--&gt;
&lt;STMTPROFILE ID="Guidelines for Q1"&gt;
	&lt;STMTKEY SCHEMA="DB2USER"&gt;
	&lt;![CDATA[SELECT SUM(S.PRICE)  
  FROM DAILY_SALES S, TIME T, PRODUCT P
  WHERE 
  T.TIMEKEY = S.TIMEKEY AND T.YEAR_MON = 200712 AND 
  P.PRODKEY = S.PRODKEY AND P.PROD_DESC = ‘Power drill’]]&gt;
	&lt;/STMTKEY&gt;
	&lt;OPTGUIDELINES&gt;
&lt;HSJOIN&gt;
		&lt;NLJOIN&gt;
			&lt;IXSCAN TABLE="T" INDEX="TD_IX1"/&gt;
			&lt;IXSCAN TABLE=”S” INDEX=”DS_TIMEKEY”/&gt;
		&lt;/NLJOIN&gt;
		&lt;ACCESS TABLE=”P” /&gt;
	&lt;/HSJOIN&gt;
	&lt;/OPTGUIDELINES&gt;
&lt;/STMTPROFILE&gt;
&lt;/OPTPROFILE&gt;
			</docbook:programlisting>
		</docbook:example>	
		<docbook:para>
This optimization profile specifies an optimization guideline for one SQL statement. The optimization guideline is represented by the 
OPTGUIDELINES element. This particular optimization guideline specifies that the TIME table should be accessed with the TD_IXI index and 
then joined using a nested-loop join to the DAILY_SALES table which will be accessed on the inner of the join using index DS_TIMEKEY. 
The result of this join will be joined to the PRODUCT table using a hash join. The PRODUCT table will be on the inner of the hash join. 
The access to the PRODUCT table is specified using the ACCESS element, which means that the optimizer is free to choose the access method.
		</docbook:para>
		<docbook:para>
Tables are referenced in the optimization guideline by using the TABLE element to specify the correlation name used to reference the tables in 
the original SQL statement. For example, the DAILY_SALES table is given the correlation name S in the original SQL statement, so you must 
reference it as S when using the TABLE element. 
		</docbook:para>
		<docbook:para>
You may also specify global optimization guidelines in the optimization profile. Global optimization guidelines specify some aspect of the SQL 
statement compilation environment, such as the optimization class, degree of parallelism, or eligible MQTs. Global optimization guidelines are 
applied to all SQL statements that are compiled while the containing optimization profile is in effect. The global optimization guideline in 
this example specifies a time limit of 1000 ms for collecting statistics at SQL statement compilation time, by the real-time statistics 
collection feature.
		</docbook:para>
		<docbook:para>
Optimization profiles are a powerful tool for controlling access plans; however, they should be used with caution. Optimization profiles 
prevent access plans from adjusting to changes in your data and your environment. While this does accomplish access plan stabilization, 
it may be a bad approach when used for extended periods of time, because the performance improvements resulting from better access plans 
will never be realized. Optimization profiles are best used for exceptional situations when the tuning actions described above are 
unsuccessful in improving or stabilizing access plans.
		</docbook:para>
		<docbook:para>
<docbook:link xlink:href="http://publib.boulder.ibm.com/infocenter/db2luw/v9r7/topic/com.ibm.db2.luw.admin.perf.doc/doc/c0024522.html">This 
DB2 Information Center page</docbook:link> provides more details on how to use optimization profiles.  .
		</docbook:para>
	</docbook:section>
	
	<docbook:section>
		<docbook:title>Conclusion</docbook:title>	
		<docbook:para>
The DB2 data server query optimizer is more likely to choose optimal access plans consistently if you provide it accurate information about 
your data and system configuration. Access plans will also tend to be more stable because they will be less sensitive to small changes in 
your data and system configuration. How you write your SQL statements also affects the optimizer’s ability to choose optimal access plans. 
Following the tuning actions described in this article should help you achieve consistently good query performance. However, for the 
exceptional situations in which the described tuning actions don’t have the desired effect, you can override the optimizer using access plan 
reuse or optimization profiles. The overall combination of system tuning actions and selective optimizer overrides should help you achieve 
your DB2 data server query performance objectives.
		</docbook:para>
	</docbook:section>	
	<!-- docbook:appendix -->
	<docbook:section>
		<!--  docbook:appendixinfo -->
			<!-- docbook:legalnotice -->
				<docbook:para>
IBM and DB2 are registered trademarks of International Business Machines Corporation in the United States, other countries, or both.
				</docbook:para>
				<docbook:para>
Linux is a registered trademark of Linus Torvalds in the United States, other countries, or both.
				</docbook:para>
				<docbook:para>
UNIX is a registered trademark of The Open Group in the United States and other countries.
				</docbook:para>
				<docbook:para>
Windows is a trademark of Microsoft Corporation in the United States, other countries, or both.
				</docbook:para>
				<docbook:para>
Other company, product, or service names may be trademarks or service marks of others.
				</docbook:para>
			</docbook:section>	
			<!-- /docbook:legalnotice -->
		<!-- /docbook:appendixinfo -->
	<!-- /docbook:appendix -->
</docbook:article>	
